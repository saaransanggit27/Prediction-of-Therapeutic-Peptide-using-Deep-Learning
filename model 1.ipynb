{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import lru_cache\n",
    "from joblib import Parallel, delayed\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Configuration\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    for device in gpu_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eb74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ProtBERT Model & Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "model = AutoModel.from_pretrained(\"Rostlab/prot_bert\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5819464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading Functions\n",
    "def load_data(folder_path, label, max_sequences=100000, chunk_size=1000):\n",
    "    sequences, labels = [], []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            for chunk in pd.read_csv(file_path, usecols=[0], chunksize=chunk_size):\n",
    "                sequences.extend(chunk.iloc[:, 0].tolist()[:max_sequences - len(sequences)])\n",
    "                labels.extend([label] * min(len(chunk), max_sequences - len(labels)))\n",
    "                if len(sequences) >= max_sequences:\n",
    "                    break\n",
    "    return sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbfca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Functions\n",
    "def extract_features(sequences):\n",
    "    def calc_feature(seq):\n",
    "        length = len(seq)\n",
    "        aa_counts = {char: seq.count(char) for char in \"AILMFWVKRDEAEHKLQRIVYFWGNPSD\"}\n",
    "        hydrophobicity = sum(aa_counts.get(char, 0) for char in \"AILMFWV\") / length\n",
    "        charge = (aa_counts.get('K', 0) + aa_counts.get('R', 0)) - (aa_counts.get('D', 0) + aa_counts.get('E', 0))\n",
    "        molecular_weight = sum(ord(char) for char in seq) / length\n",
    "        alpha_helix = sum(aa_counts.get(char, 0) for char in \"AEHKLQR\") / length\n",
    "        beta_sheet = sum(aa_counts.get(char, 0) for char in \"IVYFW\") / length\n",
    "        turn = sum(aa_counts.get(char, 0) for char in \"GNPSD\") / length\n",
    "        return [length, hydrophobicity, charge, molecular_weight, alpha_helix, beta_sheet, turn]\n",
    "    return np.array([calc_feature(seq) for seq in sequences], dtype=np.float32)\n",
    "def get_protbert_embeddings(sequences, batch_size=16):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(sequences), batch_size), desc=\"Extracting Embeddings\", unit=\"batch\"):\n",
    "            batch = [\" \".join(list(seq)) for seq in sequences[i:i + batch_size]]  \n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n",
    "    return np.vstack(embeddings).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ea158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String Algorithm Functions\n",
    "def build_suffix_array(seq):\n",
    "    \"\"\"Builds suffix array using sorted suffixes\"\"\"\n",
    "    suffixes = sorted((seq[i:], i) for i in range(len(seq)))\n",
    "    return [suffix[1] for suffix in suffixes]\n",
    "\n",
    "def build_lcp(seq, suffix_array):\n",
    "    \"\"\"Builds LCP (Longest Common Prefix) array\"\"\"\n",
    "    n = len(seq)\n",
    "    rank = [0] * n\n",
    "    lcp = [0] * n\n",
    "    for i, suffix in enumerate(suffix_array):\n",
    "        rank[suffix] = i\n",
    "    h = 0\n",
    "    for i in range(n):\n",
    "        if rank[i] > 0:\n",
    "            j = suffix_array[rank[i] - 1]\n",
    "            while (i + h < n) and (j + h < n) and (seq[i + h] == seq[j + h]):\n",
    "                h += 1\n",
    "            lcp[rank[i]] = h\n",
    "            if h > 0:\n",
    "                h -= 1\n",
    "    return lcp\n",
    "\n",
    "def compute_kmp_lps(pattern):\n",
    "    \"\"\"Computes Longest Prefix Suffix (LPS) array for KMP algorithm\"\"\"\n",
    "    lps = [0] * len(pattern)\n",
    "    j = 0  # length of previous longest prefix suffix\n",
    "    for i in range(1, len(pattern)):\n",
    "        while j > 0 and pattern[i] != pattern[j]:\n",
    "            j = lps[j - 1]\n",
    "        if pattern[i] == pattern[j]:\n",
    "            j += 1\n",
    "            lps[i] = j\n",
    "    return lps\n",
    "\n",
    "def kmp_search(sequence, pattern):\n",
    "    \"\"\"Finds occurrences of a pattern in a sequence using KMP algorithm\"\"\"\n",
    "    lps = compute_kmp_lps(pattern)\n",
    "    matches = []\n",
    "    j = 0  # index for pattern\n",
    "    for i in range(len(sequence)):\n",
    "        while j > 0 and sequence[i] != pattern[j]:\n",
    "            j = lps[j - 1]\n",
    "        if sequence[i] == pattern[j]:\n",
    "            j += 1\n",
    "        if j == len(pattern):\n",
    "            matches.append(i - j + 1)\n",
    "            j = lps[j - 1]\n",
    "    return matches\n",
    "\n",
    "# Aho-Corasick Algorithm\n",
    "class TrieNode:\n",
    "    def _init_(self):\n",
    "        self.children = {}\n",
    "        self.fail = None\n",
    "        self.output = []\n",
    "\n",
    "class AhoCorasickTrie:\n",
    "    def _init_(self, patterns):\n",
    "        self.root = TrieNode()\n",
    "        self.build_trie(patterns)\n",
    "        self.build_failure_links()\n",
    "\n",
    "    def build_trie(self, patterns):\n",
    "        \"\"\"Builds Trie from patterns\"\"\"\n",
    "        for pattern in patterns:\n",
    "            node = self.root\n",
    "            for char in pattern:\n",
    "                if char not in node.children:\n",
    "                    node.children[char] = TrieNode()\n",
    "                node = node.children[char]\n",
    "            node.output.append(pattern)\n",
    "\n",
    "    def build_failure_links(self):\n",
    "        \"\"\"Builds failure links for Aho-Corasick\"\"\"\n",
    "        queue = []\n",
    "        for node in self.root.children.values():\n",
    "            node.fail = self.root\n",
    "            queue.append(node)\n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            for char, child in current.children.items():\n",
    "                queue.append(child)\n",
    "                fail_node = current.fail\n",
    "                while fail_node and char not in fail_node.children:\n",
    "                    fail_node = fail_node.fail\n",
    "                child.fail = fail_node.children[char] if fail_node else self.root\n",
    "                child.output += child.fail.output if child.fail else []\n",
    "\n",
    "    def search(self, text):\n",
    "        \"\"\"Searches for all patterns in text\"\"\"\n",
    "        node = self.root\n",
    "        results = []\n",
    "        for i, char in enumerate(text):\n",
    "            while node and char not in node.children:\n",
    "                node = node.fail\n",
    "            if not node:\n",
    "                node = self.root\n",
    "                continue\n",
    "            node = node.children[char]\n",
    "            if node.output:\n",
    "                results.append((i, node.output))\n",
    "        return results  \n",
    "# FFT Algorithm\n",
    "def compute_fft(sequence):\n",
    "    \"\"\"Computes FFT (Fast Fourier Transform) on peptide sequence\"\"\"\n",
    "    numeric_seq = [ord(char) for char in sequence] \n",
    "    fft_result = np.fft.fft(numeric_seq)\n",
    "    return np.abs(fft_result)[:len(fft_result)//2]\n",
    "# MST Algorithm\n",
    "def prim_mst(graph):\n",
    "    \"\"\"Computes MST using Prim's Algorithm\"\"\"\n",
    "    if not graph:\n",
    "        return []\n",
    "    \n",
    "    n = len(graph)\n",
    "    mst = []\n",
    "    visited = set()\n",
    "    min_heap = [(0, 0, -1)] \n",
    "\n",
    "    while len(visited) < n:\n",
    "        cost, node, prev = heapq.heappop(min_heap)\n",
    "        if node in visited:\n",
    "            continue\n",
    "        visited.add(node)\n",
    "        if prev != -1:\n",
    "            mst.append((prev, node, cost))\n",
    "        for neighbor, weight in graph[node]:\n",
    "            if neighbor not in visited:\n",
    "                heapq.heappush(min_heap, (weight, neighbor, node))\n",
    "    return mst\n",
    "# Dynamic Programming Functions\n",
    "@lru_cache(None)\n",
    "def lcs(seq1, seq2):\n",
    "    \"\"\"Longest Common Subsequence using Dynamic Programming with memoization\"\"\"\n",
    "    if not seq1 or not seq2:\n",
    "        return 0\n",
    "    elif seq1[-1] == seq2[-1]:\n",
    "        return 1 + lcs(seq1[:-1], seq2[:-1])\n",
    "    else:\n",
    "        return max(lcs(seq1[:-1], seq2), lcs(seq1, seq2[:-1]))\n",
    "\n",
    "def compute_lcs_features(sequences):\n",
    "    \"\"\"Compute LCS features for sequences\"\"\"\n",
    "    reference_seq = sequences[0]  # Choose a reference sequence\n",
    "    return np.array([lcs(seq, reference_seq) / len(seq) for seq in sequences]).reshape(-1, 1)\n",
    "# Backtracking Function\n",
    "def generate_subsequences(seq, max_length=4):\n",
    "    \"\"\"Generate all subsequences up to max_length using backtracking\"\"\"\n",
    "    subsequences = []\n",
    "    \n",
    "    def backtrack(current, index):\n",
    "        if 1 < len(current) <= max_length:\n",
    "            subsequences.append(current)\n",
    "        if len(current) == max_length:\n",
    "            return\n",
    "        for i in range(index, len(seq)):\n",
    "            backtrack(current + seq[i], i + 1)\n",
    "    \n",
    "    backtrack(\"\", 0)\n",
    "    return subsequences\n",
    "\n",
    "def subseq_features(sequences):\n",
    "    \"\"\"Convert subsequences to features\"\"\"\n",
    "    return np.array([len(generate_subsequences(seq)) for seq in sequences]).reshape(-1, 1)\n",
    "# QuickSort for Feature Scaling\n",
    "def quicksort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    pivot = arr[len(arr) // 2]\n",
    "    left = [x for x in arr if x < pivot]\n",
    "    middle = [x for x in arr if x == pivot]\n",
    "    right = [x for x in arr if x > pivot]\n",
    "    return quicksort(left) + middle + quicksort(right)\n",
    "\n",
    "def apply_quicksort_scaling(features):\n",
    "    return np.array([quicksort(feature_row.tolist()) for feature_row in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71987b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-LSTM Model\n",
    "def create_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(input_shape, 1)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        LSTM(256, return_sequences=True, activation='tanh'),\n",
    "        Dropout(0.5),  \n",
    "        LSTM(200, return_sequences=False, activation='tanh'),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001),  \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "# Training Function\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    model = create_model(X_train.shape[1])\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True, mode='max')\n",
    "    model_checkpoint = ModelCheckpoint('Therapeutic vs Non-Therapeutic Classifier.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_val, y_val), \n",
    "                        callbacks=[early_stopping, model_checkpoint])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0af6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Function\n",
    "def predict_sequence(sequence, model_path='Therapeutic vs Non-Therapeutic Classifier.h5'):\n",
    "    model = load_model(model_path)\n",
    "    sequence_embedding = get_protbert_embeddings([sequence])\n",
    "    sequence_features = extract_features([sequence])\n",
    "    input_data = np.hstack((sequence_embedding, sequence_features)).reshape(1, -1, 1)\n",
    "    prediction = model.predict(input_data)[0]  \n",
    "    prob_therapeutic = prediction[1]\n",
    "    return \"Therapeutic\" if prob_therapeutic >= 0.5 else \"Non-Therapeutic\", prob_therapeutic\n",
    "# Load data paths\n",
    "folder_therapeutic = r\"data\\Therapeutic vs Non-Therapeutic Classification data\\Therapeutic data\"\n",
    "folder_nontherapeutic = r\"data\\Therapeutic vs Non-Therapeutic Classification data\\Non-Therapeutic data\"\n",
    "# Load dataset\n",
    "seqs_thera, labels_thera = load_data(folder_therapeutic, 1)\n",
    "seqs_nonthera, labels_nonthera = load_data(folder_nontherapeutic, 0)\n",
    "\n",
    "sequences = seqs_thera + seqs_nonthera\n",
    "labels = np.array(labels_thera + labels_nonthera, dtype=np.int32)\n",
    "# Feature extraction\n",
    "if not os.path.exists(\"X_embeddings.npy\"):\n",
    "    X_embeddings = get_protbert_embeddings(sequences)\n",
    "    np.save(\"X_embeddings.npy\", X_embeddings)\n",
    "else:\n",
    "    X_embeddings = np.load(\"X_embeddings.npy\")\n",
    "\n",
    "X_features = extract_features(sequences)\n",
    "X_combined = np.hstack((X_embeddings, X_features))\n",
    "# Reshape for CNN-LSTM\n",
    "X_combined = X_combined.reshape(X_combined.shape[0], X_combined.shape[1], 1)\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_combined, labels, test_size=0.2, random_state=42)\n",
    "# Train model\n",
    "best_model, history = train_model(X_train, y_train, X_val, y_val)\n",
    "# Plot training results\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Print final metrics\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"Final Training Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "# Evaluate on validation set\n",
    "val_loss, val_acc = best_model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
