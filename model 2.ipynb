{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import hashlib\n",
    "# ✅ Enable Full GPU Usage\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "# ✅ Load ProtBERT Model & Tokenizer\n",
    "protbert_tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\")\n",
    "protbert_model = AutoModel.from_pretrained(\"Rostlab/prot_bert\").to(device)\n",
    "# ✅ Dynamic Programming: Cache ProtBERT Embeddings\n",
    "embedding_cache = {}\n",
    "\n",
    "def hash_sequence(seq):\n",
    "    return hashlib.md5(seq.encode()).hexdigest()\n",
    "# ✅ Load Data (Greedy Approach - Read Only Needed Files)\n",
    "def load_data(base_folder):\n",
    "    sequences, labels = [], []\n",
    "    for filename in os.listdir(base_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            label = filename.replace(\".csv\", \"\")\n",
    "            file_path = os.path.join(base_folder, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            for seq in df['Sequence'].dropna():\n",
    "                sequences.append(seq)\n",
    "                labels.append(label)\n",
    "    return sequences, labels\n",
    "# ✅ Divide & Conquer: Parallel Feature Extraction\n",
    "def extract_protbert_features(sequences):\n",
    "    def process_sequence(seq):\n",
    "        seq_hash = hash_sequence(seq)\n",
    "        if seq_hash in embedding_cache:\n",
    "            return embedding_cache[seq_hash]\n",
    "\n",
    "        seq = ' '.join(list(seq))  \n",
    "        encoded = protbert_tokenizer.batch_encode_plus(\n",
    "            [seq], padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = protbert_model(**encoded).last_hidden_state.mean(dim=1).cpu().numpy().flatten()\n",
    "\n",
    "        embedding_cache[seq_hash] = output  \n",
    "        return output\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        embeddings = list(tqdm(executor.map(process_sequence, sequences), total=len(sequences), desc=\"Extracting ProtBERT Features\"))\n",
    "\n",
    "    return np.array(embeddings, dtype=np.float32)\n",
    "# ✅ Load & Preprocess Data\n",
    "base_folder = r\"data\\\\Therapeutic Category Classification\"\n",
    "sequences, labels = load_data(base_folder)\n",
    "\n",
    "protbert_features = extract_protbert_features(sequences)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(protbert_features)\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "# ✅ Feature Selection using PCA\n",
    "pca = PCA(n_components=50)  # Reduce to 50 principal components\n",
    "X_pca = pca.fit_transform(X)\n",
    "joblib.dump(pca, \"pca_model.pkl\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "# ✅ CNN-LSTM Model (Efficient)\n",
    "num_classes = len(set(labels))\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = Sequential([\n",
    "        Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(2),\n",
    "        LSTM(64),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=40, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "model.save(\"Therapeutic Peptide Classification.h5\")\n",
    "# ✅ Accuracy Evaluation\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "# ✅ Greedy Algorithm for Confidence Adjustment\n",
    "def confidence_adjustment(prediction):\n",
    "    max_prob = np.max(prediction)\n",
    "    if max_prob < 0.7:\n",
    "        return \"Uncertain Category\"\n",
    "    return label_encoder.inverse_transform([np.argmax(prediction)])[0]\n",
    "# ✅ Classify User-Entered Sequence\n",
    "def classify_peptide(sequence):\n",
    "    protbert_features = np.array(extract_protbert_features([sequence]))\n",
    "    features = scaler.transform(protbert_features)\n",
    "    features = pca.transform(features)\n",
    "    features = np.expand_dims(features, axis=-1)\n",
    "    \n",
    "    model = load_model(\"Therapeutic Peptide Classification.h5\")\n",
    "    prediction = model.predict(features)\n",
    "    category = confidence_adjustment(prediction)\n",
    "    \n",
    "    return category\n",
    "# ✅ Extract Accuracy & Loss from Model History\n",
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # ✅ Plot Accuracy Graph\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['accuracy'], 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], 'r*-', label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training & Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # ✅ Plot Loss Graph\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['loss'], 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], 'r*-', label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training & Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# ✅ Call the function with model history\n",
    "plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
